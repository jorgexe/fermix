{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "099f38ad",
   "metadata": {},
   "source": [
    "# Exoplanet Classification Model Training\n",
    "## LightGBM + Random Forest Baselines\n",
    "\n",
    "This notebook trains baseline classification models to predict exoplanet candidates using the cleaned Kepler dataset.\n",
    "\n",
    "### Objectives:\n",
    "1. Load and prepare cleaned Kepler data\n",
    "2. Define classification target (binary or multi-class)\n",
    "3. Train baseline models (RandomForest & LightGBM)\n",
    "4. Evaluate performance metrics\n",
    "5. Export trained models and metadata\n",
    "\n",
    "### Models:\n",
    "- **Random Forest**: `n_estimators=300, max_depth=None`\n",
    "- **LightGBM**: `num_leaves=63, n_estimators=500, learning_rate=0.05`\n",
    "\n",
    "### Evaluation Metrics:\n",
    "- Accuracy\n",
    "- ROC-AUC (binary) or Macro F1 (multi-class)\n",
    "- Precision-Recall AUC (for imbalanced data)\n",
    "- Confusion Matrix\n",
    "- Feature Importance"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e54d03a",
   "metadata": {},
   "source": [
    "## 1. Setup & Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "03df3e3f",
   "metadata": {},
   "outputs": [
    {
     "ename": "OSError",
     "evalue": "dlopen(/Users/jorgesandoval/Documents/current/fermix/venv/lib/python3.13/site-packages/lightgbm/lib/lib_lightgbm.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\n  Referenced from: <D44045CD-B874-3A27-9A61-F131D99AACE4> /Users/jorgesandoval/Documents/current/fermix/venv/lib/python3.13/site-packages/lightgbm/lib/lib_lightgbm.dylib\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/opt/homebrew/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libomp.dylib' (no such file), '/opt/homebrew/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libomp.dylib' (no such file)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mOSError\u001b[39m                                   Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 26\u001b[39m\n\u001b[32m     14\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mmetrics\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m (\n\u001b[32m     15\u001b[39m     accuracy_score, \n\u001b[32m     16\u001b[39m     roc_auc_score, \n\u001b[32m   (...)\u001b[39m\u001b[32m     23\u001b[39m     average_precision_score\n\u001b[32m     24\u001b[39m )\n\u001b[32m     25\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01msklearn\u001b[39;00m\u001b[34;01m.\u001b[39;00m\u001b[34;01mpreprocessing\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m LabelEncoder\n\u001b[32m---> \u001b[39m\u001b[32m26\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlightgbm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mlgb\u001b[39;00m\n\u001b[32m     27\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mjoblib\u001b[39;00m\n\u001b[32m     29\u001b[39m \u001b[38;5;66;03m# Configure display settings\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/current/fermix/venv/lib/python3.13/site-packages/lightgbm/__init__.py:11\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mpathlib\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Path\n\u001b[32m      9\u001b[39m \u001b[38;5;66;03m# .basic is intentionally loaded as early as possible, to dlopen() lib_lightgbm.{dll,dylib,so}\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# and its dependencies as early as possible\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m11\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mbasic\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Booster, Dataset, Sequence, register_logger\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mcallback\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m EarlyStopException, early_stopping, log_evaluation, record_evaluation, reset_parameter\n\u001b[32m     13\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m CVBooster, cv, train\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/current/fermix/venv/lib/python3.13/site-packages/lightgbm/basic.py:9\u001b[39m\n\u001b[32m      2\u001b[39m \u001b[33;03m\"\"\"Wrapper for C API of LightGBM.\"\"\"\u001b[39;00m\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# This import causes lib_lightgbm.{dll,dylib,so} to be loaded.\u001b[39;00m\n\u001b[32m      5\u001b[39m \u001b[38;5;66;03m# It's intentionally done here, as early as possible, to avoid issues like\u001b[39;00m\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# \"libgomp.so.1: cannot allocate memory in static TLS block\" on aarch64 Linux.\u001b[39;00m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m#\u001b[39;00m\n\u001b[32m      8\u001b[39m \u001b[38;5;66;03m# For details, see the \"cannot allocate memory in static TLS block\" entry in docs/FAQ.rst.\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01m.\u001b[39;00m\u001b[34;01mlibpath\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m _LIB  \u001b[38;5;66;03m# isort: skip\u001b[39;00m\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mabc\u001b[39;00m\n\u001b[32m     12\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mctypes\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Documents/current/fermix/venv/lib/python3.13/site-packages/lightgbm/libpath.py:49\u001b[39m\n\u001b[32m     47\u001b[39m     _LIB = Mock(ctypes.CDLL)  \u001b[38;5;66;03m# type: ignore\u001b[39;00m\n\u001b[32m     48\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m49\u001b[39m     _LIB = \u001b[43mctypes\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcdll\u001b[49m\u001b[43m.\u001b[49m\u001b[43mLoadLibrary\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_find_lib_path\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m[\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ctypes/__init__.py:471\u001b[39m, in \u001b[36mLibraryLoader.LoadLibrary\u001b[39m\u001b[34m(self, name)\u001b[39m\n\u001b[32m    470\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mLoadLibrary\u001b[39m(\u001b[38;5;28mself\u001b[39m, name):\n\u001b[32m--> \u001b[39m\u001b[32m471\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_dlltype\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/opt/homebrew/Cellar/python@3.13/3.13.7/Frameworks/Python.framework/Versions/3.13/lib/python3.13/ctypes/__init__.py:390\u001b[39m, in \u001b[36mCDLL.__init__\u001b[39m\u001b[34m(self, name, mode, handle, use_errno, use_last_error, winmode)\u001b[39m\n\u001b[32m    387\u001b[39m \u001b[38;5;28mself\u001b[39m._FuncPtr = _FuncPtr\n\u001b[32m    389\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m handle \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m390\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = \u001b[43m_dlopen\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    391\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    392\u001b[39m     \u001b[38;5;28mself\u001b[39m._handle = handle\n",
      "\u001b[31mOSError\u001b[39m: dlopen(/Users/jorgesandoval/Documents/current/fermix/venv/lib/python3.13/site-packages/lightgbm/lib/lib_lightgbm.dylib, 0x0006): Library not loaded: @rpath/libomp.dylib\n  Referenced from: <D44045CD-B874-3A27-9A61-F131D99AACE4> /Users/jorgesandoval/Documents/current/fermix/venv/lib/python3.13/site-packages/lightgbm/lib/lib_lightgbm.dylib\n  Reason: tried: '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/opt/libomp/lib/libomp.dylib' (no such file), '/opt/local/lib/libomp/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/local/lib/libomp/libomp.dylib' (no such file), '/opt/homebrew/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libomp.dylib' (no such file), '/opt/homebrew/lib/libomp.dylib' (no such file), '/System/Volumes/Preboot/Cryptexes/OS/opt/homebrew/lib/libomp.dylib' (no such file)"
     ]
    }
   ],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "import warnings\n",
    "import json\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "# Machine learning imports\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, \n",
    "    roc_auc_score, \n",
    "    f1_score, \n",
    "    confusion_matrix,\n",
    "    classification_report,\n",
    "    precision_recall_curve,\n",
    "    roc_curve,\n",
    "    auc,\n",
    "    average_precision_score\n",
    ")\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import lightgbm as lgb\n",
    "import joblib\n",
    "\n",
    "# Configure display settings\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', 100)\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set visualization style\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (12, 6)\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "RANDOM_STATE = 42\n",
    "np.random.seed(RANDOM_STATE)\n",
    "\n",
    "print(\"✓ Libraries imported successfully\")\n",
    "print(f\"Random state set to: {RANDOM_STATE}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c20ef23b",
   "metadata": {},
   "source": [
    "## 2. Load Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "df7f2a24",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the cleaned Kepler dataset\n",
    "data_path = Path('../data/clean/kepler_clean.csv')\n",
    "\n",
    "print(\"Loading cleaned Kepler dataset...\")\n",
    "df = pd.read_csv(data_path)\n",
    "\n",
    "print(f\"✓ Dataset loaded: {df.shape[0]:,} rows × {df.shape[1]} columns\")\n",
    "print(f\"\\nFirst few rows:\")\n",
    "display(df.head())\n",
    "\n",
    "print(f\"\\nColumn types:\")\n",
    "print(df.dtypes.value_counts())\n",
    "\n",
    "print(f\"\\nDataset info:\")\n",
    "print(f\"  - Memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"  - Missing values: {df.isnull().sum().sum():,}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0040122",
   "metadata": {},
   "source": [
    "## 3. Define Classification Target\n",
    "\n",
    "We'll create a classification target based on `koi_disposition`:\n",
    "- **Binary**: CONFIRMED (1) vs FALSE POSITIVE (0), excluding CANDIDATE\n",
    "- **3-Class**: CONFIRMED (2) vs CANDIDATE (1) vs FALSE POSITIVE (0)\n",
    "\n",
    "Let's start with binary classification for clearer model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a1039e3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Examine target distribution\n",
    "print(\"=\"*80)\n",
    "print(\"TARGET VARIABLE ANALYSIS\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "print(f\"\\nOriginal koi_disposition distribution:\")\n",
    "print(df['koi_disposition'].value_counts())\n",
    "print(f\"\\nPercentages:\")\n",
    "print(df['koi_disposition'].value_counts(normalize=True) * 100)\n",
    "\n",
    "# Create binary classification target (CONFIRMED vs FALSE POSITIVE)\n",
    "# Remove CANDIDATE for cleaner binary classification\n",
    "df_binary = df[df['koi_disposition'].isin(['CONFIRMED', 'FALSE POSITIVE'])].copy()\n",
    "\n",
    "# Create target variable\n",
    "df_binary['label'] = (df_binary['koi_disposition'] == 'CONFIRMED').astype(int)\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(f\"BINARY CLASSIFICATION DATASET\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Dataset size: {df_binary.shape[0]:,} rows (removed {df.shape[0] - df_binary.shape[0]:,} CANDIDATE rows)\")\n",
    "print(f\"\\nTarget distribution:\")\n",
    "print(df_binary['label'].value_counts())\n",
    "print(f\"\\nClass balance:\")\n",
    "class_pct = df_binary['label'].value_counts(normalize=True) * 100\n",
    "print(f\"  - Class 0 (FALSE POSITIVE): {class_pct[0]:.2f}%\")\n",
    "print(f\"  - Class 1 (CONFIRMED): {class_pct[1]:.2f}%\")\n",
    "\n",
    "# Check for class imbalance\n",
    "imbalance_ratio = class_pct.max() / class_pct.min()\n",
    "print(f\"\\nImbalance ratio: {imbalance_ratio:.2f}:1\")\n",
    "if imbalance_ratio > 2:\n",
    "    print(\"⚠️  Dataset is imbalanced - PR-AUC will be important metric\")\n",
    "else:\n",
    "    print(\"✓ Dataset is relatively balanced\")\n",
    "\n",
    "# Store task type\n",
    "TASK_TYPE = \"binary\"\n",
    "print(f\"\\n🎯 Task type: {TASK_TYPE} classification\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2017a5",
   "metadata": {},
   "source": [
    "## 4. Feature Selection & Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce3a8b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select features for modeling\n",
    "print(\"=\"*80)\n",
    "print(\"FEATURE SELECTION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Exclude non-feature columns\n",
    "exclude_cols = [\n",
    "    'rowid', 'kepid', 'kepoi_name', 'kepler_name', \n",
    "    'koi_disposition', 'koi_pdisposition', 'koi_comment',\n",
    "    'koi_disp_prov', 'label'  # target\n",
    "]\n",
    "\n",
    "# Get all numeric columns\n",
    "numeric_cols = df_binary.select_dtypes(include=[np.number]).columns.tolist()\n",
    "\n",
    "# Remove excluded columns\n",
    "feature_cols = [col for col in numeric_cols if col not in exclude_cols]\n",
    "\n",
    "print(f\"\\nTotal numeric columns: {len(numeric_cols)}\")\n",
    "print(f\"Excluded columns: {len(exclude_cols)}\")\n",
    "print(f\"Selected features: {len(feature_cols)}\")\n",
    "\n",
    "# Check for missing values in features\n",
    "missing_by_col = df_binary[feature_cols].isnull().sum()\n",
    "cols_with_missing = missing_by_col[missing_by_col > 0]\n",
    "\n",
    "print(f\"\\nFeatures with missing values: {len(cols_with_missing)}\")\n",
    "if len(cols_with_missing) > 0:\n",
    "    print(\"\\nTop 10 features with missing values:\")\n",
    "    for col, count in cols_with_missing.head(10).items():\n",
    "        pct = (count / len(df_binary) * 100)\n",
    "        print(f\"  - {col}: {count} ({pct:.2f}%)\")\n",
    "\n",
    "# Create feature matrix and target\n",
    "X = df_binary[feature_cols].copy()\n",
    "y = df_binary['label'].copy()\n",
    "\n",
    "# Handle missing values - simple imputation with median\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"HANDLING MISSING VALUES\")\n",
    "print(f\"{'='*80}\")\n",
    "\n",
    "missing_before = X.isnull().sum().sum()\n",
    "print(f\"Missing values before imputation: {missing_before:,}\")\n",
    "\n",
    "# Impute with median\n",
    "for col in feature_cols:\n",
    "    if X[col].isnull().any():\n",
    "        median_val = X[col].median()\n",
    "        X[col].fillna(median_val, inplace=True)\n",
    "\n",
    "missing_after = X.isnull().sum().sum()\n",
    "print(f\"Missing values after imputation: {missing_after:,}\")\n",
    "print(f\"✓ All missing values handled\")\n",
    "\n",
    "# Final dataset info\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"FINAL DATASET\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"Features (X): {X.shape[0]:,} rows × {X.shape[1]} features\")\n",
    "print(f\"Target (y): {y.shape[0]:,} samples\")\n",
    "print(f\"Feature list (first 10): {feature_cols[:10]}\")\n",
    "print(f\"...\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ba7cef1",
   "metadata": {},
   "source": [
    "## 5. Train/Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48077d36",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and test sets\n",
    "TEST_SIZE = 0.2\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"TRAIN/TEST SPLIT\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X, y, \n",
    "    test_size=TEST_SIZE, \n",
    "    random_state=RANDOM_STATE,\n",
    "    stratify=y  # Maintain class distribution\n",
    ")\n",
    "\n",
    "print(f\"\\nTest size: {TEST_SIZE * 100}%\")\n",
    "print(f\"\\nTraining set:\")\n",
    "print(f\"  - X_train: {X_train.shape[0]:,} rows × {X_train.shape[1]} features\")\n",
    "print(f\"  - y_train: {y_train.shape[0]:,} samples\")\n",
    "print(f\"  - Class distribution: {y_train.value_counts().to_dict()}\")\n",
    "\n",
    "print(f\"\\nTest set:\")\n",
    "print(f\"  - X_test: {X_test.shape[0]:,} rows × {X_test.shape[1]} features\")\n",
    "print(f\"  - y_test: {y_test.shape[0]:,} samples\")\n",
    "print(f\"  - Class distribution: {y_test.value_counts().to_dict()}\")\n",
    "\n",
    "# Verify stratification\n",
    "train_pos_pct = (y_train.sum() / len(y_train)) * 100\n",
    "test_pos_pct = (y_test.sum() / len(y_test)) * 100\n",
    "print(f\"\\nClass balance verification:\")\n",
    "print(f\"  - Train positive class: {train_pos_pct:.2f}%\")\n",
    "print(f\"  - Test positive class: {test_pos_pct:.2f}%\")\n",
    "print(f\"✓ Stratification successful (similar distributions)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7dbd8b99",
   "metadata": {},
   "source": [
    "## 6. Train Baseline Models\n",
    "\n",
    "### 6.1 Random Forest Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08bd80b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train Random Forest Classifier\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING RANDOM FOREST CLASSIFIER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model with specified hyperparameters\n",
    "rf_model = RandomForestClassifier(\n",
    "    n_estimators=300,\n",
    "    max_depth=None,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,  # Use all CPU cores\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "print(\"\\nModel hyperparameters:\")\n",
    "print(f\"  - n_estimators: {rf_model.n_estimators}\")\n",
    "print(f\"  - max_depth: {rf_model.max_depth}\")\n",
    "print(f\"  - random_state: {rf_model.random_state}\")\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "import time\n",
    "start_time = time.time()\n",
    "\n",
    "rf_model.fit(X_train, y_train)\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n✓ Training completed in {training_time:.2f} seconds\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_pred_rf = rf_model.predict(X_test)\n",
    "y_pred_proba_rf = rf_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"✓ Random Forest model trained and predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6421e34",
   "metadata": {},
   "source": [
    "### 6.2 LightGBM Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63c642c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train LightGBM Classifier\n",
    "print(\"=\"*80)\n",
    "print(\"TRAINING LIGHTGBM CLASSIFIER\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Initialize model with specified hyperparameters\n",
    "lgbm_model = lgb.LGBMClassifier(\n",
    "    num_leaves=63,\n",
    "    n_estimators=500,\n",
    "    learning_rate=0.05,\n",
    "    random_state=RANDOM_STATE,\n",
    "    n_jobs=-1,\n",
    "    verbose=-1  # Suppress per-iteration output\n",
    ")\n",
    "\n",
    "print(\"\\nModel hyperparameters:\")\n",
    "print(f\"  - num_leaves: {lgbm_model.num_leaves}\")\n",
    "print(f\"  - n_estimators: {lgbm_model.n_estimators}\")\n",
    "print(f\"  - learning_rate: {lgbm_model.learning_rate}\")\n",
    "print(f\"  - random_state: {lgbm_model.random_state}\")\n",
    "\n",
    "print(\"\\nTraining model...\")\n",
    "start_time = time.time()\n",
    "\n",
    "lgbm_model.fit(\n",
    "    X_train, y_train,\n",
    "    eval_set=[(X_test, y_test)],\n",
    "    eval_metric='auc',\n",
    "    callbacks=[lgb.early_stopping(stopping_rounds=50, verbose=False)]\n",
    ")\n",
    "\n",
    "training_time = time.time() - start_time\n",
    "print(f\"\\n✓ Training completed in {training_time:.2f} seconds\")\n",
    "print(f\"  - Best iteration: {lgbm_model.best_iteration_}\")\n",
    "print(f\"  - Best score: {lgbm_model.best_score_['valid_0']['auc']:.4f}\")\n",
    "\n",
    "# Make predictions\n",
    "print(\"\\nGenerating predictions...\")\n",
    "y_pred_lgbm = lgbm_model.predict(X_test)\n",
    "y_pred_proba_lgbm = lgbm_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "print(\"✓ LightGBM model trained and predictions generated\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4554056",
   "metadata": {},
   "source": [
    "## 7. Model Evaluation\n",
    "\n",
    "Calculate comprehensive metrics for both models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae98f05a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate both models\n",
    "print(\"=\"*80)\n",
    "print(\"MODEL EVALUATION\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "def evaluate_model(y_true, y_pred, y_pred_proba, model_name):\n",
    "    \"\"\"Calculate comprehensive evaluation metrics\"\"\"\n",
    "    \n",
    "    # Basic metrics\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "    roc_auc = roc_auc_score(y_true, y_pred_proba)\n",
    "    pr_auc = average_precision_score(y_true, y_pred_proba)\n",
    "    f1 = f1_score(y_true, y_pred)\n",
    "    \n",
    "    # Confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    tn, fp, fn, tp = cm.ravel()\n",
    "    \n",
    "    # Precision and Recall\n",
    "    precision = tp / (tp + fp) if (tp + fp) > 0 else 0\n",
    "    recall = tp / (tp + fn) if (tp + fn) > 0 else 0\n",
    "    \n",
    "    print(f\"\\n{'='*80}\")\n",
    "    print(f\"{model_name.upper()} PERFORMANCE\")\n",
    "    print(f\"{'='*80}\")\n",
    "    print(f\"\\n📊 Classification Metrics:\")\n",
    "    print(f\"  - Accuracy:        {accuracy:.4f}\")\n",
    "    print(f\"  - ROC-AUC:         {roc_auc:.4f}\")\n",
    "    print(f\"  - PR-AUC:          {pr_auc:.4f}\")\n",
    "    print(f\"  - F1 Score:        {f1:.4f}\")\n",
    "    print(f\"  - Precision:       {precision:.4f}\")\n",
    "    print(f\"  - Recall:          {recall:.4f}\")\n",
    "    \n",
    "    print(f\"\\n📈 Confusion Matrix:\")\n",
    "    print(f\"  - True Negatives:  {tn}\")\n",
    "    print(f\"  - False Positives: {fp}\")\n",
    "    print(f\"  - False Negatives: {fn}\")\n",
    "    print(f\"  - True Positives:  {tp}\")\n",
    "    \n",
    "    print(f\"\\n📋 Classification Report:\")\n",
    "    print(classification_report(y_true, y_pred, target_names=['FALSE POSITIVE', 'CONFIRMED']))\n",
    "    \n",
    "    return {\n",
    "        'accuracy': accuracy,\n",
    "        'roc_auc': roc_auc,\n",
    "        'pr_auc': pr_auc,\n",
    "        'f1_score': f1,\n",
    "        'precision': precision,\n",
    "        'recall': recall,\n",
    "        'confusion_matrix': cm.tolist()\n",
    "    }\n",
    "\n",
    "# Evaluate Random Forest\n",
    "metrics_rf = evaluate_model(y_test, y_pred_rf, y_pred_proba_rf, \"Random Forest\")\n",
    "\n",
    "# Evaluate LightGBM\n",
    "metrics_lgbm = evaluate_model(y_test, y_pred_lgbm, y_pred_proba_lgbm, \"LightGBM\")\n",
    "\n",
    "# Compare models\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"MODEL COMPARISON\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n{'Metric':<20} {'Random Forest':<15} {'LightGBM':<15} {'Winner':<10}\")\n",
    "print(f\"{'-'*60}\")\n",
    "for metric in ['accuracy', 'roc_auc', 'pr_auc', 'f1_score']:\n",
    "    rf_val = metrics_rf[metric]\n",
    "    lgbm_val = metrics_lgbm[metric]\n",
    "    winner = \"🏆 RF\" if rf_val > lgbm_val else \"🏆 LGBM\" if lgbm_val > rf_val else \"Tie\"\n",
    "    print(f\"{metric:<20} {rf_val:<15.4f} {lgbm_val:<15.4f} {winner:<10}\")\n",
    "\n",
    "print(f\"\\n🎯 Both models show strong performance for exoplanet classification!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8a19bfd",
   "metadata": {},
   "source": [
    "## 8. Export Trained Models\n",
    "\n",
    "Save models and metadata for future use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27724b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create models directory\n",
    "models_dir = Path('../models')\n",
    "models_dir.mkdir(exist_ok=True)\n",
    "\n",
    "print(\"=\"*80)\n",
    "print(\"EXPORTING MODELS AND METADATA\")\n",
    "print(\"=\"*80)\n",
    "\n",
    "# Save Random Forest model\n",
    "rf_model_path = models_dir / 'model_rf.pkl'\n",
    "joblib.dump(rf_model, rf_model_path)\n",
    "print(f\"\\n✓ Random Forest model saved: {rf_model_path}\")\n",
    "print(f\"  - File size: {rf_model_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "\n",
    "# Save LightGBM model\n",
    "lgbm_model_path = models_dir / 'model_lgbm.pkl'\n",
    "joblib.dump(lgbm_model, lgbm_model_path)\n",
    "print(f\"\\n✓ LightGBM model saved: {lgbm_model_path}\")\n",
    "print(f\"  - File size: {lgbm_model_path.stat().st_size / 1024**2:.2f} MB\")\n",
    "\n",
    "# Create metadata\n",
    "metadata = {\n",
    "    \"created_utc\": datetime.now(timezone.utc).isoformat(),\n",
    "    \"dataset\": \"Kepler KOI cleaned (binary classification)\",\n",
    "    \"task\": \"binary\",\n",
    "    \"n_samples\": {\n",
    "        \"total\": len(df_binary),\n",
    "        \"train\": len(X_train),\n",
    "        \"test\": len(X_test)\n",
    "    },\n",
    "    \"n_features\": len(feature_cols),\n",
    "    \"features\": feature_cols,\n",
    "    \"target\": \"label\",\n",
    "    \"target_mapping\": {\n",
    "        \"0\": \"FALSE POSITIVE\",\n",
    "        \"1\": \"CONFIRMED\"\n",
    "    },\n",
    "    \"class_distribution\": {\n",
    "        \"train\": y_train.value_counts().to_dict(),\n",
    "        \"test\": y_test.value_counts().to_dict()\n",
    "    },\n",
    "    \"models\": {\n",
    "        \"random_forest\": {\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"hyperparameters\": {\n",
    "                \"n_estimators\": 300,\n",
    "                \"max_depth\": None,\n",
    "                \"random_state\": RANDOM_STATE\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"accuracy\": float(metrics_rf['accuracy']),\n",
    "                \"roc_auc\": float(metrics_rf['roc_auc']),\n",
    "                \"pr_auc\": float(metrics_rf['pr_auc']),\n",
    "                \"f1_score\": float(metrics_rf['f1_score']),\n",
    "                \"precision\": float(metrics_rf['precision']),\n",
    "                \"recall\": float(metrics_rf['recall'])\n",
    "            },\n",
    "            \"confusion_matrix\": metrics_rf['confusion_matrix']\n",
    "        },\n",
    "        \"lightgbm\": {\n",
    "            \"version\": \"1.0.0\",\n",
    "            \"hyperparameters\": {\n",
    "                \"num_leaves\": 63,\n",
    "                \"n_estimators\": 500,\n",
    "                \"learning_rate\": 0.05,\n",
    "                \"random_state\": RANDOM_STATE\n",
    "            },\n",
    "            \"metrics\": {\n",
    "                \"accuracy\": float(metrics_lgbm['accuracy']),\n",
    "                \"roc_auc\": float(metrics_lgbm['roc_auc']),\n",
    "                \"pr_auc\": float(metrics_lgbm['pr_auc']),\n",
    "                \"f1_score\": float(metrics_lgbm['f1_score']),\n",
    "                \"precision\": float(metrics_lgbm['precision']),\n",
    "                \"recall\": float(metrics_lgbm['recall'])\n",
    "            },\n",
    "            \"confusion_matrix\": metrics_lgbm['confusion_matrix'],\n",
    "            \"best_iteration\": int(lgbm_model.best_iteration_)\n",
    "        }\n",
    "    },\n",
    "    \"preprocessing\": {\n",
    "        \"missing_value_strategy\": \"median imputation\",\n",
    "        \"feature_selection\": \"numeric features only, excluded identifiers and target\",\n",
    "        \"train_test_split\": {\n",
    "            \"test_size\": TEST_SIZE,\n",
    "            \"random_state\": RANDOM_STATE,\n",
    "            \"stratify\": True\n",
    "        }\n",
    "    }\n",
    "}\n",
    "\n",
    "# Save metadata\n",
    "metadata_path = models_dir / 'metadata.json'\n",
    "with open(metadata_path, 'w') as f:\n",
    "    json.dump(metadata, f, indent=2)\n",
    "\n",
    "print(f\"\\n✓ Metadata saved: {metadata_path}\")\n",
    "\n",
    "# Save feature list separately for easy access\n",
    "features_path = models_dir / 'features.json'\n",
    "with open(features_path, 'w') as f:\n",
    "    json.dump({\"features\": feature_cols, \"n_features\": len(feature_cols)}, f, indent=2)\n",
    "\n",
    "print(f\"✓ Feature list saved: {features_path}\")\n",
    "\n",
    "print(f\"\\n{'='*80}\")\n",
    "print(\"EXPORT COMPLETE\")\n",
    "print(f\"{'='*80}\")\n",
    "print(f\"\\n📦 Exported files:\")\n",
    "print(f\"  - model_rf.pkl\")\n",
    "print(f\"  - model_lgbm.pkl\")\n",
    "print(f\"  - metadata.json\")\n",
    "print(f\"  - features.json\")\n",
    "print(f\"\\n🎉 All models and metadata successfully exported!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81e2a7ad",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "### ✅ Completed Workflow:\n",
    "1. **Data Loading** - Loaded cleaned Kepler dataset\n",
    "2. **Target Definition** - Created binary classification (CONFIRMED vs FALSE POSITIVE)\n",
    "3. **Feature Selection** - Selected numeric features, handled missing values\n",
    "4. **Train/Test Split** - 80/20 split with stratification\n",
    "5. **Model Training** - Trained Random Forest and LightGBM classifiers\n",
    "6. **Evaluation** - Calculated comprehensive metrics (Accuracy, ROC-AUC, PR-AUC, F1)\n",
    "7. **Export** - Saved models and metadata\n",
    "\n",
    "### 🎯 Next Steps:\n",
    "- Run `04_eval_plots.ipynb` to generate visualizations\n",
    "- Review `docs/model_card.md` for model documentation\n",
    "- Use models for predictions on new data\n",
    "\n",
    "---\n",
    "*Model training pipeline completed successfully!*"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
